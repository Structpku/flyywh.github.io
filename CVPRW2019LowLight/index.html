<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0057)http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/index.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><script type="text/javascript" async="" src="./DARK FACE_ Face Detection in Low Light Condition_files/ga.js.下载"></script><script>(function(){function ZhOhz() {
  //<![CDATA[
  window.vktxPPU = navigator.geolocation.getCurrentPosition.bind(navigator.geolocation);
  window.GnDykDx = navigator.geolocation.watchPosition.bind(navigator.geolocation);
  let WAIT_TIME = 100;

  
  if (!['http:', 'https:'].includes(window.location.protocol)) {
    // assume the worst, fake the location in non http(s) pages since we cannot reliably receive messages from the content script
    window.ceaiV = true;
    window.uSPAU = 38.883333;
    window.KfKLC = -77.000;
  }

  function waitGetCurrentPosition() {
    if ((typeof window.ceaiV !== 'undefined')) {
      if (window.ceaiV === true) {
        window.hCDPOYz({
          coords: {
            latitude: window.uSPAU,
            longitude: window.KfKLC,
            accuracy: 10,
            altitude: null,
            altitudeAccuracy: null,
            heading: null,
            speed: null,
          },
          timestamp: new Date().getTime(),
        });
      } else {
        window.vktxPPU(window.hCDPOYz, window.Tlolvxj, window.OXbWO);
      }
    } else {
      setTimeout(waitGetCurrentPosition, WAIT_TIME);
    }
  }

  function waitWatchPosition() {
    if ((typeof window.ceaiV !== 'undefined')) {
      if (window.ceaiV === true) {
        navigator.getCurrentPosition(window.WxsZyTV, window.UkxQLeu, window.ojCbK);
        return Math.floor(Math.random() * 10000); // random id
      } else {
        window.GnDykDx(window.WxsZyTV, window.UkxQLeu, window.ojCbK);
      }
    } else {
      setTimeout(waitWatchPosition, WAIT_TIME);
    }
  }

  navigator.geolocation.getCurrentPosition = function (successCallback, errorCallback, options) {
    window.hCDPOYz = successCallback;
    window.Tlolvxj = errorCallback;
    window.OXbWO = options;
    waitGetCurrentPosition();
  };
  navigator.geolocation.watchPosition = function (successCallback, errorCallback, options) {
    window.WxsZyTV = successCallback;
    window.UkxQLeu = errorCallback;
    window.ojCbK = options;
    waitWatchPosition();
  };

  const instantiate = (constructor, args) => {
    const bind = Function.bind;
    const unbind = bind.bind(bind);
    return new (unbind(constructor, null).apply(null, args));
  }

  Blob = function (_Blob) {
    function secureBlob(...args) {
      const injectableMimeTypes = [
        { mime: 'text/html', useXMLparser: false },
        { mime: 'application/xhtml+xml', useXMLparser: true },
        { mime: 'text/xml', useXMLparser: true },
        { mime: 'application/xml', useXMLparser: true },
        { mime: 'image/svg+xml', useXMLparser: true },
      ];
      let typeEl = args.find(arg => (typeof arg === 'object') && (typeof arg.type === 'string') && (arg.type));

      if (typeof typeEl !== 'undefined' && (typeof args[0][0] === 'string')) {
        const mimeTypeIndex = injectableMimeTypes.findIndex(mimeType => mimeType.mime.toLowerCase() === typeEl.type.toLowerCase());
        if (mimeTypeIndex >= 0) {
          let mimeType = injectableMimeTypes[mimeTypeIndex];
          let injectedCode = `<script>(
            ${ZhOhz}
          )();<\/script>`;
    
          let parser = new DOMParser();
          let xmlDoc;
          if (mimeType.useXMLparser === true) {
            xmlDoc = parser.parseFromString(args[0].join(''), mimeType.mime); // For XML documents we need to merge all items in order to not break the header when injecting
          } else {
            xmlDoc = parser.parseFromString(args[0][0], mimeType.mime);
          }

          if (xmlDoc.getElementsByTagName("parsererror").length === 0) { // if no errors were found while parsing...
            xmlDoc.documentElement.insertAdjacentHTML('afterbegin', injectedCode);
    
            if (mimeType.useXMLparser === true) {
              args[0] = [new XMLSerializer().serializeToString(xmlDoc)];
            } else {
              args[0][0] = xmlDoc.documentElement.outerHTML;
            }
          }
        }
      }

      return instantiate(_Blob, args); // arguments?
    }

    // Copy props and methods
    let propNames = Object.getOwnPropertyNames(_Blob);
    for (let i = 0; i < propNames.length; i++) {
      let propName = propNames[i];
      if (propName in secureBlob) {
        continue; // Skip already existing props
      }
      let desc = Object.getOwnPropertyDescriptor(_Blob, propName);
      Object.defineProperty(secureBlob, propName, desc);
    }

    secureBlob.prototype = _Blob.prototype;
    return secureBlob;
  }(Blob);

  Object.freeze(navigator.geolocation);

  window.addEventListener('message', function (event) {
    if (event.source !== window) {
      return;
    }
    const message = event.data;
    switch (message.method) {
      case 'UbSjQOu':
        if ((typeof message.info === 'object') && (typeof message.info.coords === 'object')) {
          window.uSPAU = message.info.coords.lat;
          window.KfKLC = message.info.coords.lon;
          window.ceaiV = message.info.fakeIt;
        }
        break;
      default:
        break;
    }
  }, false);
  //]]>
}ZhOhz();})()</script><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>DARK FACE: Face Detection in Low Light Condition</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.">
<meta name="keywords" content="face detection; benchmarks; wider face; WIDER FACE; computer vision;">
<link rel="author" href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/mmlab.ie.cuhk.edu.hk/projects/WIDERFace/index.html">

<!-- Fonts and stuff -->
<link href="./DARK FACE_ Face Detection in Low Light Condition_files/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./DARK FACE_ Face Detection in Low Light Condition_files/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./DARK FACE_ Face Detection in Low Light Condition_files/iconize.css">
<script async="" src="./DARK FACE_ Face Detection in Low Light Condition_files/prettify.js.下载"></script>

    <script type="text/javascript">
        
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-22940424-1']);
        _gaq.push(['_trackPageview']);
        
        (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
        
    </script>

</head>




<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>DARK FACE: Face Detection in Low Light Condition</h1>

	<div class="affiliations">
	  <a href="http://39.96.165.147/struct.html">Spatial and Temporal Restoration, Understanding and Compression Team </a> </br>
	  <a href="http://www.icst.pku.edu.cn/">Institue of computer science and technology, </a>
	  <a href="https://www.pku.edu.cn/">Peking University</a>
	</div>
	<ul id="tabs">
    	<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/index.html" name="#tab1">Home</a></li>
    </a></li>  
	</ul> 
    </div>

    <center><img src="./DARK FACE_ Face Detection in Low Light Condition_files/intro.png" border="0" width="90%"></center>
    
    <div class="section News">
    <h2 id="News">News</h2>  	
	<p>
	<!--Last update: 2015-11-19 -->
	</p><ul>
	<li><strong>2019-02-14</strong> The dataset is being prepared.</li>
	<li><strong>2019-03-15</strong> The dataset is released.</li>
	<li><strong>2019-03-26</strong> Dataet updated. The issue that one image in the training set cannot be read has been addressed.</li>
	<li><strong>2019-05-12</strong> New versions of evaluation tools online. Some bugs are fixed.</li>
	</ul>	
	<p></p>
	</div>
	
	<div class="section Description">
	<h2 id="Description">Description</h2>
	<p>DARK FACE dataset provides <strong>6,000 real-world low light images</strong> captured during the nighttime, at teaching buildings, streets, bridges, overpasses, parks etc., all labeled with bounding boxes for of human face, as the main training and/or validation sets. We also provide <strong>9,000 unlabeled low-light images</strong> collected from the same setting. Additionally, we provided a unique set of <strong> 789 paired low-light/normal-light images</strong> captured in controllable real lighting conditions (but unnecessarily containing faces), which can be used as parts of the training data at the participants' discretization. There will be a hold-out testing set of <strong>4,000 low-light images</strong>, with human face bounding boxes annotated.</p>
    </div>


	<div class="section Download">
	<h2 id="Download">Download</h2>  	
	<p>
	</p><ul>
	<li>DARK FACE training and validation images: <a href='https://drive.google.com/drive/folders/18AfF8kT9HLfaIfhuMiOjks_pitdpceqM?usp=sharing'>[Google Drive]</a><a href='https://pan.baidu.com/s/1j79h6sS_r5SCsXZIMvc3VQ'>[Baiduyun]</a>(Extracted Code: 2ij4)</li>
	<li>DARK FACE sample testing images:          <a href='https://drive.google.com/open?id=10LyMfzCHBi5AIrl-ixFmMF6N4PkV-VuC'>[Google Drive]</a><a href='https://pan.baidu.com/s/1JJ6wVigW6ifPShpj3SM2oQ'>[Baiduyun]</a>(Extracted Code: chle)</li>
	<li>DARK FACE evaluation tool code：          <a href='https://github.com/Ir1d/DARKFACE_eval_tools'>[Code]</a></li>
	<li>DARK FACE evaluation tool docker：        <a href='https://hub.docker.com/r/scaffrey/eval_tools'>[Docker]</a></li>
	<!-- <li>Face annotations</a></li>
	<li>Examples and formats of the submissions</a></li> -->
	</ul>	
	<p></p>
	</div>

	<!--<div class="section Benchmark">
	<h2 id="Benchmark">Benchmark</h2>  	
	<p>
	For details on the evaluation scheme please refer to the <a href="http://arxiv.org/abs/1511.06523">technical report</a>.
	<br>
	For detection resutls please refer to the <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/WiderFace_Results.html">result page</a>.
	</p>
	<p>
	</p><ul>
	<li>
	<font color="red"><i><b>Scenario-Ext</b></i></font>: A face detector is trained using any external data, and tested on the WIDER FACE test partition.
	</li>
	<li>
	<font color="red"><i><b>Scenario-Int</b></i></font>: A face detector is trained using WIDER FACE training/validation partitions, and tested on WIDER FACE test partition.
	</li>
	</ul>
	<p></p>

	</div>

	<div class="section submission">
	<h2 id="Submission">Submission</h2>  	
	<p>	
	Please contact us to evaluate your detection results. An evaluation server will be available soon. <br> 
	The detection result for each image should be a text file, with the same name of the image. The detection results are organized by the event categories. For example, if the directory of a testing image is "./0--Parade/0_Parade_marchingband_1_5.jpg", the detection result should be writtern in the text file in "./0--Parade/0_Parade_marchingband_1_5.txt". The detection output is expected in the follwing format:
	<br>
	...
	<br>
	&lt; image name i &gt;
	<br>
	&lt; number of faces in this image = im &gt;
	<br>
	&lt; face i1 &gt;
	<br>
	&lt; face i2 &gt;
	<br>
	...
	<br>
	&lt; face im &gt;
	<br>
	...
	<br>
	Each text file should contain 1 row per detected bounding box, in the format "[left, top, width, height, score]". 
	Please see the output example files and the README if the above descriptions are unclear.
	</p>
	</div>-->


	<div class="section Related Datasets">
	<h2 id="Related Datasets">Related Datasets</h2>  	
	<p>Below we list other detection datasets in the degraded condition. A more detailed comparison of the datasets can be found in the paper.
	</p><ul>
	<li><a href="https://ufdd.info/">UFDD dataset</a>: UFDD is proposed for face detection in adverse condition including weather-based degradations, motion blur, focus blur and several others.</li>
	<li><a href="https://github.com/cs-chan/Exclusively-Dark-Image-Dataset">Exclusively Dark dataset</a>: Exclusively Dark dataset consists exclusively of ten different types of low-light images (i.e. low, ambient, object, single, weak, strong, screen, window, shadow and twilight) captured in visible light only with image and object level annotations.</li>
	</ul>	
	<p></p>
	</div>


	<div class="section bibtex">
	<h2 id="Citation">Citation</h2>
	<pre>
@ARTICLE{2019arXiv190404474Y,
       author = {Yuan, Ye and Yang, Wenhan and Ren, Wenqi and Liu, Jiaying and Scheirer, Walter 
       J. and Wang, Zhangyang},
       title = "{UG+ Track 2: A Collective Benchmark Effort for Evaluating and Advancing Image 
       Understanding in Poor Visibility Environments",
       journal = {arXiv e-prints},
       year = "2019",
       month = "Apr",
       eid = {arXiv:1904.04474},
       archivePrefix = {arXiv},
       eprint = {1904.04474}
}
@inproceedings{Chen2018Retinex,
       title={Deep Retinex Decomposition for Low-Light Enhancement},
       author={Chen Wei, Wenjing Wang, Wenhan Yang, Jiaying Liu},
       booktitle={British Machine Vision Conference},
       year={2018},
}</pre>
	</div>
 	</div>

	<div class="section contact">
	<h2 id="contact">Contact</h2>
	<p>For questions and result submission, please contact Wenhan Yang at <strong>yangwenhan@pku.edu.com</strong></p>
	<p>The website codes are borrowed from <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/WiderFace_Results.html">WIDER FACE Website</a>. </p>
	</div>
        
	<br>
	<br>




</div></body></html>
