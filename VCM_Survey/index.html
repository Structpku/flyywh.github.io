<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0052)https://flyywh.github.io/Single_rain_removal_survey/ -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>Video Coding for Machine</title>
<link rel="stylesheet" type="text/css" href="./project.css">
<script type="text/javascript" src="./MathJax.js.下载">
</script>
</head>

<body>
<div id="main">
  
	<div class="content"><br>
		<div align="center" id="res"> 
	    	<h1>Video Coding for Machines: A Paradigm of Collaborative Compression and Intelligent Analytics</h1>
		</div>
		<div class="authors"> &nbsp &nbsp &nbsp &nbsp &nbsp Ling-Yu Duan, Jiaying Liu, Wenhan Yang, Tiejun Huang, Wen Gao </div>
		<br>
	  	<p class="banner" align="center">arXiv, January, 2020. </p>


		<div class="Abstract sec">
		<h2>Abstract</h2>

		<div align="center" id="res"> 
                <p> Video coding, which targets to compress and reconstruct the whole frame, and feature compression, which only preserves and transmits the most critical information, stand at two ends of the scale. That is, one is with compactness and efficiency to serve for machine vision, and the other is with full fidelity, bowing to human perception. The recent endeavors in imminent trends of video compression, e.g. deep learning based coding tools and end-to-end image/video coding, and MPEG-7 compact feature descriptor standards, i.e. Compact Descriptors for Visual Search and Compact Descriptors for Video Analysis, promote the sustainable and fast development in their own directions, respectively. In this paper, thanks to booming AI technology, e.g. prediction and generation models, we carry out exploration in the new area, Video Coding for Machines (VCM), arising from the emerging MPEG standardization efforts1. Towards collaborative compression and intelligent analytics, VCM attempts to bridge the gap between feature coding for machine vision and video coding for human vision. Aligning with the rising Analyze then Compress instance Digital Retina, the definition, formulation, and paradigm of VCM are given first. Meanwhile, we systematically review state-of-the-art techniques in video compression and feature compression from the unique perspective of MPEG standardization, which provides the academic and industrial evidence to realize the collaborative compression of video and feature streams in a broad range of AI applications. Finally, we come up with potential VCM solutions, and the preliminary results have demonstrated the performance and efficiency gains. Further direction is discussed as well.
		</p>
		</div>
			
        <div align="center" id="res"> 
            <img src="./r1.gif" width="24%">
            <img src="./r2.gif" width="24%">
            <img src="./r3.gif" width="24%">
            <img src="./r4.gif" width="24%">
        </div> 
        <p>Fig. 1. Video reconstruction results of different methods.</p> <br>
        
        <div align="center" id="res"> 
            <img src="./visual_results.png" width="100%">
        </div> 
        <p>Fig. 2. Example frames of different compression methods.</p> <br>

        
        <h2> Deep Intermediate Feature Compression</h2>
        <h3> 1) Intermediate Feature Compression for Different Tasks </h3>
        <p> We compare the performance of compressing the intermediate features for different tasks. The results are shown in Tables I-III. 
            The compression rate is inferred with the volume of original intermediate deep features and the corresponding bit-streams.
            As to the fidelity evaluation, the reconstructed features are passed to their birth-layer of the corresponding neural network to infer the network outputs, which will be compared with pristine outputs to evaluate the information loss of the lossy compression methods.
            More details to calculate these two metrics can be found in [107]. </p>
        <div align="center" id="res"> 
            <img src="./exp1.png" width="32%">
            &nbsp;&nbsp;
            <img src="./exp2.png" width="32%">
            &nbsp;&nbsp;
            <img src="./exp3.png" width="32%">
        </div> <br>
        <div align="center" id="res"> 
            
        </div>
        
        <h3> 2) Channel Packaging </h3>
        <p> Deep features have multiple channels. It needs to decide how to arrange these features into single-channel or three-channel maps and then compress them with the existing video codecs. Three modes are provided: default channel concatenation,channel concatenation by descending difference, and channel tiling. The results by the three ways to package the channel are presented in Table IV-VI.</p>
        <div align="center" id="res"> 
            <img src="./exp4.png" width="46%">
            &nbsp;&nbsp;&nbsp;&nbsp;
            <img src="./exp5.png" width="46%">
        </div> <br>

        <h2> Joint Compression of Feature and Video </h2>
        <h3> 1) Quatative Results </h3>
        <div align="center" id="res"> 
            <img src="./SSIM.png" width="45%">
            &nbsp;&nbsp;&nbsp;&nbsp;
            <img src="./action_recognition.png" width="50%">
        </div> <br>
        <h3> 2) Human Detection </h3>
        <div align="center" id="res"> 
            <img src="./human_detection.png" width="50%">
            &nbsp;&nbsp;&nbsp;&nbsp;
            <img src="./Subjective_result.png" width="45%">
        </div> <br>
        <h3> 3) Reconstruction Quality Changes & RD-Curve </h3>
        <div align="center" id="res">
            <img src="./quality_changes.png" width="48%">
            &nbsp;&nbsp;&nbsp;&nbsp;
            <img src="./RD_curve.png" width="47%">
        </div> <br>
        
        <style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:5px 15px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:0px 0px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-s6z2{text-align:center}
</style>
	
</div>

		<div class="download sec">
			<h2>Download</h2>
			<div>
				<li><strong><a href="index.html">Paper</a></strong>  </li>
                <li><strong><a href="index.html">Slide</a></strong>  </li>
				<li><strong><a href="index.html">Results</a></strong>  </li>
			</div>
		</div>

        
		<div class="download sec">
			<h2>Reference</h2>
			<div>
				<p>[107] Z. Chen, K. Fan, S. Wang, L.-Y. Duan, W. Lin, and A. Kot, "Lossy intermediate deep learning feature compression and evaluation," in ACM MM, 2019.</p>
			</div>
		</div>


<!-- 		<div class="experiments sec">
			<h2>Additional Results</h2>
			<div id="images">
				<h3>Action Detection Performance</h3>
				<div align="center" id="table">
					<p>Table 1. F 1−Score on OAD dataset</p>
					<img src='OAD/figures/OAD_table.png' width='50%' >
				</div>
			</div>
		</div>
		<br></br> -->


<div class="citation sec">
    <h2>&nbsp;</h2>
    </div>
  </div>
</div>



</body></html>
